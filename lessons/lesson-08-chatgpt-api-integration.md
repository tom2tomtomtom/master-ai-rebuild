# Lesson 8: ChatGPT API Integration - Build Custom AI Applications
*Embed GPT-5 intelligence into your products, workflows, and business systems for scalable AI automation*

## ðŸ’° The Business Reality

ChatGPT's web interface is powerful but doesn't scale for business automation or product integration:
- **Manual bottleneck**: Every ChatGPT interaction requires a human to type prompts and copy-paste outputsâ€”impossible to process thousands of requests
- **No system integration**: ChatGPT can't automatically trigger from your CRM, pull data from your database, or push results to your tools
- **Product limitation**: SaaS companies can't offer AI features to customers without building their own models (historically $500K+ investment)

The OpenAI API (powered by GPT-5 as of August 2025) lets you programmatically access ChatGPT intelligence, enabling automated workflows, custom applications, and AI product features at $0.002-$0.06 per request.

**Today's ROI:**
- Time saved: Automate 100-1,000+ AI requests/day that would each take human 5-10 minutes
- Money saved: $20,000-100,000+ vs. building custom AI models or hiring ML team
- Capability gained: AI product features that generate recurring revenue or competitive differentiation

---

## âš¡ Your First API Call in 60 Seconds

Let's make an API call to GPT-5 and get a response programmatically.

### Prerequisites:
- OpenAI account (sign up at platform.openai.com)
- API key (generate at platform.openai.com/api-keys)

### Copy This Exact Code:

**Python:**
```python
import openai

# Set your API key
openai.api_key = "YOUR_API_KEY_HERE"

# Make API call
response = openai.chat.completions.create(
    model="gpt-5",
    messages=[
        {"role": "system", "content": "You are a helpful business assistant."},
        {"role": "user", "content": "Write a professional email subject line for a sales outreach about project management software."}
    ]
)

# Print result
print(response.choices[0].message.content)
```

**JavaScript (Node.js):**
```javascript
const OpenAI = require('openai');

const openai = new OpenAI({
  apiKey: 'YOUR_API_KEY_HERE'
});

async function main() {
  const completion = await openai.chat.completions.create({
    model: "gpt-5",
    messages: [
      {"role": "system", "content": "You are a helpful business assistant."},
      {"role": "user", "content": "Write a professional email subject line for a sales outreach about project management software."}
    ]
  });

  console.log(completion.choices[0].message.content);
}

main();
```

**cURL (Command Line):**
```bash
curl https://api.openai.com/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer YOUR_API_KEY_HERE" \
  -d '{
    "model": "gpt-5",
    "messages": [
      {"role": "system", "content": "You are a helpful business assistant."},
      {"role": "user", "content": "Write a professional email subject line for a sales outreach about project management software."}
    ]
  }'
```

### Customize:
- YOUR_API_KEY_HERE: Get from platform.openai.com/api-keys
- Change the user message to any task you need

### What You Should See:
```
"Streamline Your Team's Projects: See 30% Faster Delivery with [Product Name]"
```

### Success Indicator:
âœ… You received a GPT-5 response programmatically (not through web interface)â€”you can now scale this to thousands of requests

---

## ðŸŽ“ Progressive Mastery

### Foundation Level (5 minutes)
*Master the API request structure and core parameters*

### Exercise 1: Advanced Parameter Tuning - The 3 Pillars of API Success

The core of successful API integration lies in mastering the parameters that control the AI's behavior. We will explore three distinct use cases, each representing a pillar of business application: **Consistency (Customer Service), Creativity (Marketing), and Accuracy (Data Analysis)**. Your mission is to understand how to tune the `model`, `temperature`, and `max_tokens` for each specific business goal.

#### What You're Learning (5 âœ… principles)
âœ… **The Cost-Quality Trade-off:** Understanding how model choice (`gpt-5` vs. `gpt-5-mini`) directly impacts both output quality and operational cost.
âœ… **The Consistency-Creativity Spectrum:** Mastering the `temperature` parameter to ensure outputs are either reliably consistent (low temp) or highly creative (high temp).
âœ… **The System Message as a Constraint:** Using the `system` role to define the AI's persona, rules, and output format, which is critical for automation.
âœ… **Token Management for Efficiency:** Controlling response length with `max_tokens` to manage latency and optimize per-request cost.
âœ… **The Use Case-Parameter Mapping:** Developing an intuition for which parameters are most critical for different business functions (e.g., low temperature for finance, high temperature for advertising).

---

#### Template 1: High-Volume, High-Consistency Automation (The Customer Service Bot)

| Field | Content |
| :--- | :--- |
| **Name** | Customer Support Ticket Triage and Response |
| **When to use** | <ul><li>Processing thousands of daily requests.</li><li>When cost-efficiency is the top priority.</li><li>When responses must be strictly on-brand and policy-compliant.</li><li>For tasks requiring deterministic, repeatable output (e.g., summarization).</li><li>When the task is simple and does not require complex reasoning.</li></ul> |
| **Setup Prompt** | **System:** You are a Tier 1 customer support agent for [SAAS_COMPANY_NAME], a project management software. Your tone must be professional, empathetic, and solution-focused. Always acknowledge the issue, provide a solution or next steps, and reference the help documentation. Never promise refunds over $100 or admit liability without checking policy. **User:** Customer email: My account was charged twice this month. Need refund ASAP. |
| **Practice Scenario** | A mid-sized e-commerce company receives **1,500** customer support tickets per day. They aim to automate **80%** of initial responses with an AI bot. The average cost per manual response is **$2.50**. Using the `gpt-5-mini` model (costing **$0.002** per 1K tokens), calculate the potential daily savings and set the parameters for maximum consistency. |
| **Success Metrics** | <ul><li>âœ… Model is set to `gpt-5-mini` for cost optimization.</li><li>âœ… `temperature` is set to **0.2** or lower for maximum consistency.</li><li>âœ… `max_tokens` is capped at **300** to control response length and cost.</li><li>âœ… The system message clearly defines the AI's role, tone, and constraints.</li><li>âœ… The output is a professional, empathetic response to the customer's billing issue.</li><li>âœ… The response avoids promising a refund over $100.</li><li>âœ… The response suggests the next step (e.g., "I've escalated this to our billing team").</li><li>âœ… The estimated daily savings are calculated correctly (Manual cost: $3,750; Automated cost: ~$10; Savings: ~$3,740).</li></ul> |

---

#### Template 2: High-Quality, High-Creativity Generation (The Marketing Copywriter)

| Field | Content |
| :--- | :--- |
| **Name** | Ad Headline and Campaign Ideation |
| **When to use** | <ul><li>Generating multiple, diverse creative options.</li><li>When the goal is to capture attention and provoke thought.</li><li>For tasks where originality and novelty are highly valued.</li><li>When testing different messaging angles for A/B testing.</li><li>For brainstorming product names, slogans, or taglines.</li></ul> |
| **Setup Prompt** | **System:** You are a creative copywriter for a B2B SaaS company. Your brand voice is bold, data-driven, and slightly provocative. Your task is to write compelling ad headlines that challenge the status quo. **User:** Product: AI-powered sales automation. Target: Sales managers at 50-500 person companies. Pain point: Too much manual data entry. Generate 5 unique headlines. |
| **Practice Scenario** | A marketing team needs to generate **50** unique ad headlines for a new product launch. They plan to A/B test the top **5** performers. They have a budget of **$500** for content generation. Using the `gpt-5` model (costing **$0.06** per 1K tokens), set the parameters to maximize creative diversity and ensure the headlines are short and punchy. |
| **Success Metrics** | <ul><li>âœ… Model is set to `gpt-5` for highest quality and creative capacity.</li><li>âœ… `temperature` is set to **1.0** or higher for maximum creativity.</li><li>âœ… `max_tokens` is capped at **100** to ensure short-form headline output.</li><li>âœ… The system message clearly defines the creative persona and brand voice.</li><li>âœ… The output contains at least 5 distinct and provocative headlines.</li><li>âœ… The headlines are relevant to the target audience (Sales Managers).</li><li>âœ… The headlines address the pain point (manual data entry).</li><li>âœ… The `top_p` parameter is set to a high value (e.g., **0.9**) to encourage diverse word choice.</li><li>âœ… The estimated cost for 50 headlines is well within the $500 budget (e.g., 50 requests * 100 tokens/request * $0.06/1K tokens = $0.30).</li></ul> |

---

#### Template 3: Zero-Creativity, High-Accuracy Reasoning (The Data Analyst)

| Field | Content |
| :--- | :--- |
| **Name** | Financial Data Analysis and Reporting |
| **When to use** | <ul><li>Analyzing structured data (CSV, JSON, tables).</li><li>For tasks requiring mathematical precision and logical deduction.</li><li>When the output must be factual and non-hallucinatory.</li><li>For code generation, debugging, or complex problem-solving.</li><li>When the reasoning process is more important than the final answer.</li></ul> |
| **Setup Prompt** | **System:** You are a senior financial data analyst. Analyze the provided data methodically, show your work, and present the final conclusion in a JSON object. **User:** Dataset: [paste 10 rows of financial data]. Question: Calculate the Q3 2025 Net Profit Margin and identify the top 3 expense categories. |
| **Practice Scenario** | A finance team needs to analyze a quarterly expense report containing **5,000** data points. The analysis must be **100%** accurate and include a detailed breakdown of the calculation steps. They expect the final report to be approximately **1,500** tokens long. Using the `o1` model (costing **$0.15** per 1K tokens), set the parameters to eliminate creativity and ensure a comprehensive, structured output. |
| **Success Metrics** | <ul><li>âœ… Model is set to `o1` (Reasoning Specialist) for analytical tasks.</li><li>âœ… `temperature` is set to **0.0** for zero creativity and maximum accuracy.</li><li>âœ… `max_tokens` is set to **1500** or higher to accommodate the detailed report.</li><li>âœ… The system message mandates a methodical analysis and a structured output format (JSON).</li><li>âœ… The output includes the calculated Net Profit Margin.</li><li>âœ… The output correctly identifies the top 3 expense categories.</li><li>âœ… The output shows the step-by-step reasoning for the calculations.</li><li>âœ… The response is entirely factual and contains no speculative or creative language.</li><li>âœ… The estimated cost for the single report is calculated correctly (e.g., 1,500 tokens * $0.15/1K tokens = $0.225).</li></ul> |

---

#### Try It Now (7 steps)
1. **Select a Template:** Choose one of the three templates above (Customer Service, Marketing, or Data Analysis).
2. **Set Up Your Environment:** Ensure you have your OpenAI API key and the necessary Python or Node.js environment set up from the previous section.
3. **Copy the Code:** Use the base Python code structure provided in the lesson.
4. **Tune the Parameters:** Adjust the `model`, `temperature`, and `max_tokens` in your code to match the specific settings recommended in your chosen template.
5. **Refine the System Prompt:** Copy the **System** and **User** content from the template's **Setup Prompt** into the `messages` array in your code.
6. **Execute the Code:** Run the script and observe the output.
7. **Verify the Metrics:** Check the output against the **Success Metrics** list for your chosen template.

#### Final Success Metric
**You have successfully tuned the API parameters to achieve a specific business outcome (Consistency, Creativity, or Accuracy), demonstrating mastery over the core controls of the ChatGPT API.**


### Intermediate Level (7 minutes)
*Build production-ready integrations with error handling and optimization*

**Exercise 2: Production-Ready API Integration Pattern**

**Scenario:** You're building an AI feature into your product. The API integration needs to handle errors gracefully, retry failures, track costs, and perform reliably under load. A basic API call isn't enoughâ€”you need production-quality engineering.

**Your Mission:** Learn the patterns for robust, scalable API integration.

**Production API Wrapper:**

```python
import openai
import time
import logging
from typing import Dict, List, Optional

class GPTClient:
    def __init__(self, api_key: str, model: str = "gpt-5-mini"):
        self.client = openai.OpenAI(api_key=api_key)
        self.model = model
        self.request_count = 0
        self.total_tokens = 0
        logging.basicConfig(level=logging.INFO)

    def chat_completion(
        self,
        messages: List[Dict[str, str]],
        temperature: float = 0.7,
        max_tokens: int = 500,
        max_retries: int = 3
    ) -> Optional[str]:
        """
        Make API call with automatic retry logic and error handling
        """
        for attempt in range(max_retries):
            try:
                response = self.client.chat.completions.create(
                    model=self.model,
                    messages=messages,
                    temperature=temperature,
                    max_tokens=max_tokens
                )

                # Track usage
                self.request_count += 1
                self.total_tokens += response.usage.total_tokens

                # Log for debugging
                logging.info(f"Request {self.request_count}: {response.usage.total_tokens} tokens")

                return response.choices[0].message.content

            except openai.RateLimitError:
                # Hit rate limit, wait and retry
                wait_time = 2 ** attempt  # Exponential backoff
                logging.warning(f"Rate limit hit. Waiting {wait_time}s...")
                time.sleep(wait_time)

            except openai.APIError as e:
                # API error (500, 503, etc.)
                logging.error(f"API error: {e}")
                if attempt < max_retries - 1:
                    time.sleep(2 ** attempt)
                else:
                    raise

            except Exception as e:
                # Unexpected error
                logging.error(f"Unexpected error: {e}")
                raise

        return None  # All retries failed

    def get_usage_stats(self) -> Dict:
        """Track cost and usage"""
        cost_per_1k = 0.002  # GPT-5-mini pricing
        estimated_cost = (self.total_tokens / 1000) * cost_per_1k

        return {
            "requests": self.request_count,
            "total_tokens": self.total_tokens,
            "estimated_cost_usd": round(estimated_cost, 4)
        }

# Usage
gpt = GPTClient(api_key="your_key_here", model="gpt-5-mini")

response = gpt.chat_completion(
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Summarize this customer feedback: [text]"}
    ],
    temperature=0.3,
    max_tokens=200
)

print(response)
print(gpt.get_usage_stats())  # Track costs
```

**Key Production Patterns:**

**1. Retry Logic with Exponential Backoff**
```python
# Don't just failâ€”retry with increasing delays
for attempt in range(max_retries):
    try:
        response = api_call()
        break
    except RateLimitError:
        wait = 2 ** attempt  # 1s, 2s, 4s, 8s...
        time.sleep(wait)
```

**Why:** API failures are often transient (rate limits, brief outages). Exponential backoff prevents hammering the API.

**2. Usage Tracking**
```python
# Track tokens and cost per request
self.total_tokens += response.usage.total_tokens
estimated_cost = (self.total_tokens / 1000) * cost_per_1k
```

**Why:** API costs scale with usage. Track early to avoid surprise bills.

**3. Logging and Monitoring**
```python
logging.info(f"Request successful: {tokens} tokens")
logging.error(f"Request failed: {error}")
```

**Why:** Debugging production issues requires logs. Know what failed and why.

**4. Error Handling by Type**
```python
try:
    response = api_call()
except RateLimitError:
    # Retry with backoff
except AuthenticationError:
    # Invalid API keyâ€”don't retry
except APIError:
    # Server errorâ€”retry
except InvalidRequestError:
    # Bad inputâ€”don't retry, log for debugging
```

**Why:** Different errors need different handling. Don't retry authentication errors; do retry rate limits.

**Real-World Integration Examples:**

**Example 1: Batch Processing (Process 1,000 customer emails)**
```python
import time
from concurrent.futures import ThreadPoolExecutor, as_completed

class BatchProcessor:
    def __init__(self, api_key: str):
        self.gpt = GPTClient(api_key, model="gpt-5-mini")

    def process_email_batch(self, emails: List[str], max_workers: int = 5):
        """
        Process multiple emails in parallel with rate limiting
        """
        results = []

        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # Submit all tasks
            future_to_email = {
                executor.submit(self.process_single_email, email): email
                for email in emails
            }

            # Process results as they complete
            for future in as_completed(future_to_email):
                email = future_to_email[future]
                try:
                    result = future.result()
                    results.append(result)
                except Exception as e:
                    logging.error(f"Failed to process email: {e}")
                    results.append({"email": email, "error": str(e)})

                # Rate limiting: pause between batches
                time.sleep(0.1)  # 10 requests/second

        return results

    def process_single_email(self, email_text: str) -> Dict:
        """Classify email sentiment and urgency"""
        response = self.gpt.chat_completion(
            messages=[
                {"role": "system", "content": """
Analyze customer emails. Output JSON:
{
  "sentiment": "positive/neutral/negative",
  "urgency": "low/medium/high/critical",
  "category": "billing/technical/sales/other",
  "summary": "one-sentence summary"
}
                """},
                {"role": "user", "content": email_text}
            ],
            temperature=0.2,
            max_tokens=150
        )

        import json
        return json.loads(response)

# Process 1,000 emails
processor = BatchProcessor(api_key="your_key")
results = processor.process_email_batch(customer_emails)

# Route based on results
for result in results:
    if result["urgency"] == "critical":
        alert_human_immediately(result)
    elif result["sentiment"] == "negative":
        route_to_senior_support(result)
    else:
        route_to_standard_queue(result)
```

**Example 2: Real-Time API (User-facing feature in web app)**
```python
from flask import Flask, request, jsonify

app = Flask(__name__)
gpt_client = GPTClient(api_key="your_key", model="gpt-5-mini")

@app.route('/api/generate-headline', methods=['POST'])
def generate_headline():
    """
    API endpoint: User provides product info, get marketing headline
    """
    try:
        data = request.json
        product_name = data.get('product_name')
        key_benefit = data.get('key_benefit')
        target_audience = data.get('target_audience')

        # Validate input
        if not all([product_name, key_benefit, target_audience]):
            return jsonify({"error": "Missing required fields"}), 400

        # Call GPT API
        response = gpt_client.chat_completion(
            messages=[
                {"role": "system", "content": "You are a marketing copywriter. Write compelling 10-word headlines."},
                {"role": "user", "content": f"""
Product: {product_name}
Key benefit: {key_benefit}
Target audience: {target_audience}

Write 3 headline options, each under 10 words.
                """}
            ],
            temperature=0.9,  # Creative
            max_tokens=100
        )

        return jsonify({
            "headlines": response.split('\n'),
            "usage": gpt_client.get_usage_stats()
        })

    except Exception as e:
        logging.error(f"Headline generation failed: {e}")
        return jsonify({"error": "Internal server error"}), 500

# Run server
if __name__ == '__main__':
    app.run(debug=True)
```

**Usage:**
```bash
curl -X POST http://localhost:5000/api/generate-headline \
  -H "Content-Type: application/json" \
  -d '{
    "product_name": "TaskFlow Pro",
    "key_benefit": "30% faster project delivery",
    "target_audience": "Project managers at tech companies"
  }'
```

**Example 3: Webhook Integration (Trigger on external events)**
```python
from flask import Flask, request
import requests

app = Flask(__name__)
gpt_client = GPTClient(api_key="your_key")

@app.route('/webhook/new-customer', methods=['POST'])
def new_customer_webhook():
    """
    Triggered when new customer signs up (from Stripe, Salesforce, etc.)
    Auto-generate personalized onboarding email
    """
    customer_data = request.json

    # Generate personalized email
    email_body = gpt_client.chat_completion(
        messages=[
            {"role": "system", "content": """
You are a customer success manager. Write warm, personalized onboarding emails.
Tone: Friendly but professional. Include: welcome, quick start guide, support info.
            """},
            {"role": "user", "content": f"""
New customer:
- Name: {customer_data['name']}
- Company: {customer_data['company']}
- Plan: {customer_data['plan']}
- Use case: {customer_data['use_case']}

Write onboarding email (150 words).
            """}
        ],
        temperature=0.7,
        max_tokens=300
    )

    # Send email via your email service
    send_email(
        to=customer_data['email'],
        subject=f"Welcome to {YOUR_PRODUCT}, {customer_data['name']}!",
        body=email_body
    )

    logging.info(f"Onboarding email sent to {customer_data['email']}")

    return jsonify({"status": "success"}), 200
```

**Pro Technique: Streaming Responses**

For real-time UX (like ChatGPT's typing effect):

```python
response = openai.chat.completions.create(
    model="gpt-5",
    messages=[{"role": "user", "content": "Write a story"}],
    stream=True  # Enable streaming
)

# Get response word-by-word as it's generated
for chunk in response:
    if chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end='', flush=True)
```

**Use case:** User-facing chat interfaces where you want to show response as it's generated (better perceived performance).

---

### Advanced Level (8 minutes)
*Enterprise-scale API architecture and optimization*

**Exercise 3: Building Production AI Features at Scale**

**Scenario:** Your SaaS product is adding AI features used by thousands of customers generating millions of API requests per month. You need architecture that handles scale, controls costs, maintains performance, and provides reliability guarantees.

**Your Mission:** Design enterprise-grade API integration with caching, rate limiting, cost controls, and monitoring.

**Enterprise API Architecture:**

```python
import os
import redis
import hashlib
import json
from typing import Optional, Dict
from datetime import datetime, timedelta
import openai

class EnterpriseGPTClient:
    """
    Production-grade GPT API client with:
    - Response caching (reduce redundant API calls)
    - Rate limiting (prevent bill shock)
    - Cost tracking per user/tenant
    - Fallback models (cost optimization)
    - Monitoring and alerting
    """

    def __init__(
        self,
        api_key: str,
        redis_url: str = "redis://localhost:6379",
        monthly_budget_usd: float = 1000.0
    ):
        self.client = openai.OpenAI(api_key=api_key)
        self.cache = redis.from_url(redis_url)
        self.monthly_budget = monthly_budget_usd
        self.cost_tracker = CostTracker(redis_client=self.cache)

    def smart_completion(
        self,
        messages: List[Dict],
        user_id: str,
        cache_ttl: int = 3600,  # 1 hour cache
        fallback_to_mini: bool = True
    ) -> Dict:
        """
        Intelligent API call with caching, cost optimization, and rate limiting
        """

        # 1. Check cache first (avoid unnecessary API calls)
        cache_key = self._generate_cache_key(messages)
        cached_response = self.cache.get(cache_key)
        if cached_response:
            logging.info("Cache hit - no API call needed")
            return json.loads(cached_response)

        # 2. Check user rate limit
        if not self._check_rate_limit(user_id):
            raise Exception(f"Rate limit exceeded for user {user_id}")

        # 3. Check budget
        current_spend = self.cost_tracker.get_monthly_spend()
        if current_spend >= self.monthly_budget:
            logging.warning("Monthly budget exceeded")
            # Option A: Block requests
            # raise Exception("Monthly budget exceeded")
            # Option B: Auto-fallback to cheaper model
            if fallback_to_mini:
                logging.info("Falling back to gpt-5-mini")
                model = "gpt-5-mini"
            else:
                raise Exception("Budget exceeded and fallback disabled")
        else:
            model = "gpt-5"

        # 4. Make API call
        try:
            response = self.client.chat.completions.create(
                model=model,
                messages=messages,
                temperature=0.7,
                max_tokens=500
            )

            result = {
                "content": response.choices[0].message.content,
                "model": model,
                "tokens": response.usage.total_tokens,
                "cost": self._calculate_cost(response.usage.total_tokens, model)
            }

            # 5. Track cost
            self.cost_tracker.record_request(
                user_id=user_id,
                tokens=result["tokens"],
                cost=result["cost"]
            )

            # 6. Cache result
            self.cache.setex(
                cache_key,
                cache_ttl,
                json.dumps(result)
            )

            return result

        except Exception as e:
            logging.error(f"API call failed: {e}")
            raise

    def _generate_cache_key(self, messages: List[Dict]) -> str:
        """Generate deterministic cache key from messages"""
        messages_str = json.dumps(messages, sort_keys=True)
        return f"gpt_cache:{hashlib.md5(messages_str.encode()).hexdigest()}"

    def _check_rate_limit(self, user_id: str, max_per_minute: int = 20) -> bool:
        """Rate limit per user (prevent abuse)"""
        key = f"rate_limit:{user_id}:{datetime.now().strftime('%Y%m%d%H%M')}"
        current = self.cache.get(key)

        if current and int(current) >= max_per_minute:
            return False

        self.cache.incr(key)
        self.cache.expire(key, 60)  # Reset after 1 minute
        return True

    def _calculate_cost(self, tokens: int, model: str) -> float:
        """Calculate cost based on model pricing"""
        pricing = {
            "gpt-5": 0.06 / 1000,
            "gpt-5-mini": 0.002 / 1000,
            "gpt-4o": 0.03 / 1000
        }
        return tokens * pricing.get(model, 0.06 / 1000)


class CostTracker:
    """Track API costs per user and globally"""

    def __init__(self, redis_client):
        self.cache = redis_client

    def record_request(self, user_id: str, tokens: int, cost: float):
        """Record API usage"""
        # Global monthly spend
        month_key = f"cost:global:{datetime.now().strftime('%Y%m')}"
        self.cache.incrbyfloat(month_key, cost)

        # Per-user spend
        user_month_key = f"cost:user:{user_id}:{datetime.now().strftime('%Y%m')}"
        self.cache.incrbyfloat(user_month_key, cost)

        # Per-user token count
        user_token_key = f"tokens:user:{user_id}:{datetime.now().strftime('%Y%m')}"
        self.cache.incrby(user_token_key, tokens)

    def get_monthly_spend(self) -> float:
        """Get total spend this month"""
        month_key = f"cost:global:{datetime.now().strftime('%Y%m')}"
        spend = self.cache.get(month_key)
        return float(spend) if spend else 0.0

    def get_user_spend(self, user_id: str) -> Dict:
        """Get user's usage this month"""
        month = datetime.now().strftime('%Y%m')
        cost = self.cache.get(f"cost:user:{user_id}:{month}") or 0
        tokens = self.cache.get(f"tokens:user:{user_id}:{month}") or 0

        return {
            "user_id": user_id,
            "month": month,
            "cost_usd": float(cost),
            "tokens": int(tokens)
        }


# Usage in production
gpt = EnterpriseGPTClient(
    api_key=os.environ["OPENAI_API_KEY"],
    redis_url=os.environ["REDIS_URL"],
    monthly_budget_usd=5000.0
)

# Make request (with caching, rate limiting, cost tracking)
result = gpt.smart_completion(
    messages=[
        {"role": "system", "content": "You are a customer support agent."},
        {"role": "user", "content": "How do I reset my password?"}
    ],
    user_id="user_12345",
    cache_ttl=3600  # Cache for 1 hour
)

print(result["content"])
print(f"Cost: ${result['cost']:.4f}")
```

**Key Enterprise Patterns:**

**1. Response Caching**
```python
# Hash the request â†’ check cache â†’ return if exists â†’ otherwise API call
cache_key = hash(messages)
if cache.get(cache_key):
    return cached_response  # No API call = $0 cost
```

**Why:** Identical requests (FAQs, common queries) waste money. Cache = 100% cost savings for repeated requests.

**Typical savings:** 30-50% cost reduction

**2. Automatic Cost Controls**
```python
if monthly_spend >= budget:
    # Option A: Block requests (protect budget)
    # Option B: Fall back to cheaper model (gpt-5-mini)
    # Option C: Alert and continue
```

**Why:** Prevent surprise $10K bills. Budget guardrails are essential.

**3. Per-User Rate Limiting**
```python
if requests_this_minute[user] > 20:
    raise RateLimitError("Slow down")
```

**Why:** Prevent abuse, ensure fair usage, protect API quota.

**4. Cost Attribution**
```python
# Track cost per user, per department, per feature
track_cost(user_id="user123", feature="ai_summary", cost=0.003)
```

**Why:** Understand which users/features drive costs. Optimize or charge accordingly.

**Advanced Use Cases:**

**Use Case 1: Multi-Tenant SaaS with Usage-Based Billing**

```python
class TenantGPTService:
    """
    SaaS with AI features: Track usage per customer, bill accordingly
    """

    def __init__(self, api_key: str):
        self.gpt = EnterpriseGPTClient(api_key)

    def process_request(self, tenant_id: str, request_data: Dict) -> Dict:
        """
        Process AI request, track cost, bill customer
        """
        # Make API call
        result = self.gpt.smart_completion(
            messages=request_data["messages"],
            user_id=f"tenant_{tenant_id}",
            cache_ttl=1800
        )

        # Record billable usage
        self.record_billable_usage(
            tenant_id=tenant_id,
            feature="ai_content_generation",
            cost=result["cost"],
            units=1  # 1 generation = 1 billable unit
        )

        return {
            "result": result["content"],
            "credits_used": 1,
            "remaining_credits": self.get_remaining_credits(tenant_id)
        }

    def record_billable_usage(self, tenant_id: str, feature: str, cost: float, units: int):
        """Track usage for billing system"""
        # Write to database for billing
        db.execute("""
            INSERT INTO usage_records (tenant_id, feature, cost, units, timestamp)
            VALUES (%s, %s, %s, %s, NOW())
        """, (tenant_id, feature, cost, units))

        # Update tenant's usage counter
        db.execute("""
            UPDATE tenants
            SET ai_credits_used = ai_credits_used + %s
            WHERE tenant_id = %s
        """, (units, tenant_id))

# Usage
service = TenantGPTService(api_key=os.environ["OPENAI_API_KEY"])

result = service.process_request(
    tenant_id="acme_corp",
    request_data={
        "messages": [
            {"role": "user", "content": "Generate marketing email"}
        ]
    }
)

# Result includes usage tracking for billing
print(result["remaining_credits"])  # Show customer their usage
```

**Use Case 2: AI Feature Gating (Free vs. Paid Tiers)**

```python
class FeatureGateGPTService:
    """
    Different AI capabilities for different subscription tiers
    """

    TIER_LIMITS = {
        "free": {"requests_per_day": 10, "model": "gpt-5-mini", "max_tokens": 200},
        "pro": {"requests_per_day": 500, "model": "gpt-5", "max_tokens": 1000},
        "enterprise": {"requests_per_day": 10000, "model": "gpt-5", "max_tokens": 4000}
    }

    def process_with_tier_limits(self, user_id: str, user_tier: str, messages: List[Dict]) -> Dict:
        """Apply tier-based limits"""
        limits = self.TIER_LIMITS.get(user_tier, self.TIER_LIMITS["free"])

        # Check daily limit
        daily_usage = self.get_daily_usage(user_id)
        if daily_usage >= limits["requests_per_day"]:
            return {
                "error": "Daily limit reached",
                "upgrade_url": "/upgrade-to-pro",
                "limit": limits["requests_per_day"]
            }

        # Use tier-appropriate model
        response = self.gpt.client.chat.completions.create(
            model=limits["model"],
            messages=messages,
            max_tokens=limits["max_tokens"]
        )

        # Increment usage
        self.increment_daily_usage(user_id)

        return {
            "content": response.choices[0].message.content,
            "tier": user_tier,
            "daily_usage": daily_usage + 1,
            "daily_limit": limits["requests_per_day"]
        }
```

**Use Case 3: Automated Content Pipeline**

```python
class ContentPipeline:
    """
    Multi-stage AI pipeline: Research â†’ Outline â†’ Write â†’ Edit â†’ SEO
    """

    def __init__(self, api_key: str):
        self.gpt = EnterpriseGPTClient(api_key)

    def generate_blog_post(self, topic: str) -> Dict:
        """
        Fully automated content creation pipeline
        """

        # Stage 1: Research (cheap model)
        research = self.gpt.smart_completion(
            messages=[
                {"role": "system", "content": "Research assistant. Find key facts and angles."},
                {"role": "user", "content": f"Research topic: {topic}. List 10 key points."}
            ],
            user_id="system_pipeline",
            model="gpt-5-mini"
        )

        # Stage 2: Create outline (cheap model)
        outline = self.gpt.smart_completion(
            messages=[
                {"role": "system", "content": "Content strategist. Create article outlines."},
                {"role": "user", "content": f"Topic: {topic}\nResearch: {research['content']}\nCreate detailed outline."}
            ],
            user_id="system_pipeline",
            model="gpt-5-mini"
        )

        # Stage 3: Write article (premium model for quality)
        article = self.gpt.smart_completion(
            messages=[
                {"role": "system", "content": "Professional writer. Write engaging 1,500-word articles."},
                {"role": "user", "content": f"Outline: {outline['content']}\nWrite complete article."}
            ],
            user_id="system_pipeline",
            model="gpt-5"  # Use best model for final content
        )

        # Stage 4: SEO optimization (cheap model)
        seo = self.gpt.smart_completion(
            messages=[
                {"role": "system", "content": "SEO specialist. Generate meta descriptions, titles, keywords."},
                {"role": "user", "content": f"Article: {article['content']}\nGenerate SEO elements."}
            ],
            user_id="system_pipeline",
            model="gpt-5-mini"
        )

        return {
            "article": article["content"],
            "seo_title": seo["title"],
            "meta_description": seo["meta"],
            "keywords": seo["keywords"],
            "total_cost": sum([research["cost"], outline["cost"], article["cost"], seo["cost"]])
        }

# Generate 100 blog posts overnight
pipeline = ContentPipeline(api_key=os.environ["OPENAI_API_KEY"])

for topic in content_calendar:
    post = pipeline.generate_blog_post(topic)
    publish_to_cms(post)
```

**Pro Technique: Model Fallback Cascade**

```python
def robust_completion_with_fallback(messages: List[Dict]) -> str:
    """
    Try premium model first, fall back to cheaper if unavailable
    """
    models_to_try = [
        ("gpt-5", 0.06),
        ("gpt-4o", 0.03),
        ("gpt-5-mini", 0.002)
    ]

    for model, cost_per_1k in models_to_try:
        try:
            response = openai.chat.completions.create(
                model=model,
                messages=messages,
                timeout=30
            )
            return response.choices[0].message.content
        except (openai.RateLimitError, openai.APIError):
            logging.warning(f"{model} failed, trying next model...")
            continue

    raise Exception("All models failed")
```

**Why:** Reliability over perfection. If GPT-5 is down or rate-limited, automatically fall back to GPT-4o, then gpt-5-mini. Availability > model quality.

---

## ðŸš€ Production Templates

*Save these for immediate business use*

### Template 1: Customer Support Auto-Response

```python
def auto_respond_to_ticket(ticket_text: str, customer_history: Dict) -> str:
    """
    Automatically draft response to customer support ticket
    """
    response = openai.chat.completions.create(
        model="gpt-5-mini",  # Cost-effective for support
        messages=[
            {"role": "system", "content": f"""
You are a Tier 1 support agent for {COMPANY_NAME}.

Context:
- Product: {PRODUCT_DESCRIPTION}
- Customer tier: {customer_history['tier']}
- Previous issues: {customer_history['past_issues']}

Task: Draft professional response that:
1. Acknowledges their issue with empathy
2. Provides solution or next steps
3. References help docs when relevant
4. Sets timeline expectations

Tone: Professional, empathetic, solution-focused
Never: Promise refunds over $100, admit liability
            """},
            {"role": "user", "content": ticket_text}
        ],
        temperature=0.3,  # Consistent responses
        max_tokens=300
    )

    return response.choices[0].message.content

# Usage
ticket = "My account was charged twice this month"
customer = db.get_customer(customer_id)

draft_response = auto_respond_to_ticket(ticket, customer)
present_to_agent_for_review(draft_response)  # Human reviews before sending
```

**Use When:**
- High-volume support tickets
- Want faster first response time
- Need consistent tone across team

**Time Saved:** 10-15 min per ticket (faster drafting)
**Cost:** $0.0003-0.0005 per ticket

---

### Template 2: Data Extraction from Unstructured Text

```python
def extract_structured_data(text: str) -> Dict:
    """
    Extract structured data from unstructured text (emails, forms, documents)
    """
    response = openai.chat.completions.create(
        model="gpt-5-mini",
        messages=[
            {"role": "system", "content": """
Extract structured data from text. Output valid JSON only.

Schema:
{
  "company_name": "string",
  "contact_person": "string",
  "email": "string",
  "phone": "string",
  "product_interest": "string",
  "budget_range": "string",
  "timeline": "string"
}

If field not found, use null.
            """},
            {"role": "user", "content": text}
        ],
        temperature=0.0,  # Deterministic extraction
        max_tokens=300
    )

    import json
    return json.loads(response.choices[0].message.content)

# Usage: Extract lead info from inbound email
email_body = """
Hi, I'm Sarah from Acme Corp. We're interested in your project management
software for our 50-person team. Budget is around $5K annually.
Looking to implement in Q1 2026. You can reach me at sarah@acmecorp.com
or 555-0123.
"""

lead_data = extract_structured_data(email_body)
crm.create_lead(lead_data)  # Auto-populate CRM
```

**Use When:**
- Processing form submissions, emails, documents
- Extracting data for CRM, database
- Eliminating manual data entry

**Time Saved:** 5-10 min per document (vs. manual entry)
**Accuracy:** 95%+ (human review for critical data)

---

### Template 3: Content Generation at Scale

```python
def generate_seo_content_batch(topics: List[str]) -> List[Dict]:
    """
    Generate 100+ SEO articles automatically
    """
    articles = []

    for topic in topics:
        response = openai.chat.completions.create(
            model="gpt-5",  # Quality matters for published content
            messages=[
                {"role": "system", "content": """
You are an SEO content writer. Write 1,200-word articles:
- Clear structure (intro, 5-7 sections, conclusion)
- SEO-optimized (natural keyword usage, headers)
- Engaging and informative
- Include actionable takeaways
                """},
                {"role": "user", "content": f"Topic: {topic}\n\nWrite complete article with meta description."}
            ],
            temperature=0.8,  # Creative but on-topic
            max_tokens=2000
        )

        articles.append({
            "topic": topic,
            "content": response.choices[0].message.content,
            "word_count": len(response.choices[0].message.content.split()),
            "tokens_used": response.usage.total_tokens
        })

        time.sleep(1)  # Rate limiting

    return articles

# Generate month's content in one run
topics = ["AI in Healthcare", "Remote Work Tools 2026", "Email Marketing Tips"]
articles = generate_seo_content_batch(topics)

for article in articles:
    publish_to_blog(article)
```

**Use When:**
- Content marketing at scale
- SEO content farms
- Product descriptions, landing pages

**Time Saved:** 3-4 hours per article (vs. human writer)
**Cost:** $0.10-0.20 per article (vs. $100-500 human writer)

---

### Template 4: Intelligent Routing/Classification

```python
def classify_and_route(message: str) -> Dict:
    """
    Classify messages and route to appropriate handler
    """
    response = openai.chat.completions.create(
        model="gpt-5-mini",
        messages=[
            {"role": "system", "content": """
Classify messages into categories. Output JSON:
{
  "category": "sales_inquiry/support_request/billing_issue/spam/other",
  "urgency": "low/medium/high/critical",
  "sentiment": "positive/neutral/negative",
  "suggested_assignee": "sales_team/support_tier1/support_tier2/billing_dept"
}
                """},
            {"role": "user", "content": message}
        ],
        temperature=0.0,
        max_tokens=100
    )

    import json
    classification = json.loads(response.choices[0].message.content)

    # Route based on classification
    routing = {
        "sales_inquiry": route_to_sales_team,
        "support_request": route_to_support,
        "billing_issue": route_to_billing,
        "spam": mark_as_spam
    }

    handler = routing.get(classification["category"], route_to_general)
    handler(message, classification)

    return classification

# Usage: Classify inbound emails
for email in inbox:
    classification = classify_and_route(email.body)
    log_classification(email.id, classification)
```

**Use When:**
- Email routing, ticket triage
- Lead qualification
- Content moderation

**Time Saved:** 30-60 min/day (vs. manual sorting)
**Accuracy:** 90-95% correct routing

---

### Template 5: Personalization at Scale

```python
def personalize_email_campaign(template: str, prospects: List[Dict]) -> List[Dict]:
    """
    Personalize email template for 1,000+ prospects
    """
    personalized_emails = []

    for prospect in prospects:
        response = openai.chat.completions.create(
            model="gpt-5-mini",
            messages=[
                {"role": "system", "content": """
Personalize email templates based on prospect data.
Maintain core message but adapt:
- Opening line (reference their company, industry, role)
- Value prop (emphasize most relevant benefit)
- Social proof (cite similar customer if available)
Keep length similar to template.
                """},
                {"role": "user", "content": f"""
Template:
{template}

Prospect:
- Name: {prospect['name']}
- Company: {prospect['company']}
- Role: {prospect['role']}
- Industry: {prospect['industry']}
- Pain point: {prospect['pain_point']}

Personalize email.
                """}
            ],
            temperature=0.7,
            max_tokens=400
        )

        personalized_emails.append({
            "to": prospect['email'],
            "subject": generate_subject_line(prospect),  # Separate call
            "body": response.choices[0].message.content,
            "prospect_id": prospect['id']
        })

    return personalized_emails

# Usage: Send 1,000 personalized cold emails
template = load_email_template("cold_outreach_v2.txt")
prospects = db.get_prospects(status="new", limit=1000)

emails = personalize_email_campaign(template, prospects)
send_via_email_service(emails)
```

**Use When:**
- Cold email outreach
- Newsletter personalization
- Account-based marketing

**Time Saved:** 10-15 min per email (vs. manual personalization)
**Response Rate:** 2-3x higher than generic emails

---

## ðŸ’¼ Business Integration

### This Week: Quick Wins (30 minutes)

**Immediate Actions:**

1. **Get API Access (5 minutes)**
   - Sign up at platform.openai.com
   - Add payment method
   - Generate API key (store securely)
   - Set spending limit ($100 max to start)

2. **Test Basic API Call (10 minutes)**
   - Use cURL or Python/JavaScript example from Quick Win section
   - Verify you can get a response
   - Try modifying the prompt
   - Check usage at platform.openai.com/usage

3. **Identify One Automation Opportunity (15 minutes)**
   - Find one repetitive task you do often (email drafting, data extraction, content generation)
   - Estimate how many times per week (10x? 100x?)
   - Calculate time per task (5 min? 30 min?)
   - Sketch out API integration (input â†’ API call â†’ output)

**Expected Results:**
- Working API access
- First successful API call
- One automation opportunity identified with ROI estimate

---

### This Month: Strategic Implementation (4 weeks)

**Week 1-2: Build First Integration**
- Choose highest-impact use case from your list
- Develop basic integration (use production templates as starting point)
- Test with 10-20 real examples
- Measure time saved vs. manual process
- Calculate cost per API call

**Milestone:** One working automation processing real tasks

**Week 3-4: Optimize and Scale**
- Add error handling, retry logic
- Implement caching for repeated requests
- Set up usage tracking and alerts
- Scale to handle full volume
- Train team on using integration

**Milestone:** Automation handling 80%+ of target use case; measurable ROI

---

### Team Rollout: Organizational Scale

**Phase 1 - Proof of Concept (Week 1-2):**
- Who: Technical team (developers, IT)
- What: Build 1-2 API integrations for high-volume tasks
- Training: API documentation, code examples
- Success metric: Working integration with positive ROI

**Phase 2 - Expand Use Cases (Week 3-6):**
- Who: Product, engineering, operations teams
- What: Identify 5-10 automation opportunities across departments
- Resources: Shared API wrapper library, production templates
- Success metric: 5+ integrations deployed, $5K+/month time savings

**Phase 3 - Enterprise Deployment (Month 3+):**
- Expansion: API integrations across all departments
- Governance: API key management, budget controls, monitoring
- Support: Developer documentation, internal API best practices
- Measurement: Track usage, cost, time saved per integration
- Iterate: Optimize high-cost integrations, deprecate low-value ones

**Success Metrics:**
- **Integrations:** 20+ API-powered automations
- **Volume:** 10K+ API calls per month
- **ROI:** 10:1 (time savings value vs. API costs)
- **Adoption:** 70%+ of eligible use cases automated

**Common Objections & Responses:**

**"API costs will be unpredictable/expensive"**
â†’ Set hard spending limits in OpenAI dashboard ($500/month cap). Implement usage tracking (see Enterprise GPT Client example). Start small (100 API calls) to measure actual costs. Most use cases cost $0.001-0.01 per request (pennies). Example: 10,000 customer emails processed = $20-30 in API costs vs. $5,000 in human time.

**"We don't have developers to build integrations"**
â†’ No-code options exist: Zapier has OpenAI integration, Make.com supports API calls. For custom needs: hire freelance developer for $500-1,000 to build initial integration, then internal team maintains. Or use production templates from this lesson (copy-paste ready).

**"Security concerns with sending data to OpenAI"**
â†’ Valid. Options: 1) OpenAI Enterprise plan (zero data retention, no training on your data, HIPAA/SOC 2 compliant), 2) Redact sensitive data before API call (names, SSNs, etc.), 3) For highly sensitive: consider Azure OpenAI Service (data stays in your Azure environment). Most business data (support tickets, marketing content, public info) is safe for standard API.

**"What if OpenAI changes pricing or shuts down API?"**
â†’ Pricing has been stable since 2023 and trending down (GPT-4 was $0.03â†’$0.01 per 1K tokens). API is OpenAI's primary revenue stream (billions in annual recurring revenue)â€”not going away. Mitigation: design integrations with model-agnostic code (can swap in Anthropic, Google APIs with minor changes). Don't over-optimize for one provider.

---

## ðŸ”§ Troubleshooting & Pro Tips

### Common Issues & Solutions:

**Problem 1: "Rate limit exceeded" errors**
**Solution:**
- Check your tier limits at platform.openai.com/account/limits
- Free tier: 20 requests/minute, Paid tier (>$5 spent): 3,500/minute
- Implement exponential backoff retry (see production code example)
- Request rate limit increase at platform.openai.com/account/limits
- Consider batching requests (process 100 at once vs. 100 separate calls)

**Why it happens:** OpenAI limits requests per minute to prevent abuse and ensure service reliability

---

**Problem 2: "This model is currently overloaded" errors**
**Solution:**
- Implement automatic retry with exponential backoff
- Add model fallback: try GPT-5 â†’ GPT-4o â†’ gpt-5-mini
- Consider switching to Azure OpenAI (separate capacity, SLA guarantees)
- Retry after 5-10 seconds usually succeeds

**Why it happens:** High demand on specific model, temporary capacity issues

---

**Problem 3: API costs higher than expected**
**Solution:**
- Implement caching (see Enterprise GPT Client exampleâ€”saves 30-50%)
- Use gpt-5-mini for simple tasks ($0.002 vs. $0.06 = 97% savings)
- Reduce max_tokens parameter (shorter responses = lower cost)
- Monitor usage daily at platform.openai.com/usage
- Set billing alerts at $50, $100, $500 thresholds

**Why it happens:** High volume Ã— expensive model Ã— long responses = compounding costs

---

**Problem 4: Responses are inconsistent or not following instructions**
**Solution:**
- Lower temperature (0.0-0.3 for consistency)
- Add more specific instructions in system message
- Include examples of correct output format
- Use JSON mode for structured output: `response_format={"type": "json_object"}`
- Test with 20+ diverse inputs to identify failure patterns

**Why it happens:** Vague instructions + high temperature = unpredictable outputs

---

**Problem 5: Authentication errors (401 Unauthorized)**
**Solution:**
- Verify API key is correct (copy-paste from platform.openai.com)
- Check API key hasn't expired (regenerate if old)
- Ensure proper header format: `Authorization: Bearer sk-...`
- Check for spaces or newlines in API key string
- Verify API key has billing method attached

**Why it happens:** Invalid/expired API key or incorrect authentication format

---

### Pro Tips from Experts:

**ðŸ’¡ Power Technique 1: JSON Mode for Structured Output**
Force GPT to return valid JSON:
```python
response = openai.chat.completions.create(
    model="gpt-5-mini",
    messages=[
        {"role": "system", "content": "Extract data. Output JSON with fields: name, email, company"},
        {"role": "user", "content": text}
    ],
    response_format={"type": "json_object"}  # Guarantees valid JSON
)

data = json.loads(response.choices[0].message.content)  # No parsing errors
```

**When to use:** Data extraction, structured output, API integrations requiring JSON
**Expected impact:** 99% valid JSON (vs. 80% with prompt-only instructions)

---

**ðŸ’¡ Power Technique 2: Parallel API Calls for Speed**
Process multiple requests simultaneously:
```python
from concurrent.futures import ThreadPoolExecutor

def process_batch(items: List[str]) -> List[str]:
    with ThreadPoolExecutor(max_workers=10) as executor:
        results = list(executor.map(api_call, items))
    return results

# 100 items: Sequential = 100s, Parallel = 10s
```

**When to use:** Batch processing (hundreds of items), time-sensitive workflows
**Expected impact:** 10x faster processing

---

**ðŸ’¡ Power Technique 3: Prompt Caching (Beta)**
Reuse large system prompts across requests:
```python
# Cache expensive system prompt (10,000 tokens of product docs)
# Pay once for caching, then 90% discount on subsequent uses
```

**When to use:** Large system prompts (documentation, examples) used repeatedly
**Expected impact:** 50-90% cost reduction on cached content

---

**ðŸ’¡ Power Technique 4: Token Counting Before API Call**
Prevent expensive requests:
```python
import tiktoken

def count_tokens(text: str, model: str = "gpt-5") -> int:
    encoding = tiktoken.encoding_for_model(model)
    return len(encoding.encode(text))

# Check before API call
if count_tokens(user_input) > 10000:
    truncate_or_reject(user_input)
```

**When to use:** User-generated input, prevent abuse, cost control
**Expected impact:** Eliminate unexpectedly expensive requests

---

**ðŸ’¡ Power Technique 5: Model Routing Logic**
Use cheapest model that meets quality threshold:
```python
def smart_model_selection(task_complexity: str) -> str:
    routing = {
        "simple": "gpt-5-mini",      # Classification, extraction
        "moderate": "gpt-4o",         # Writing, analysis
        "complex": "gpt-5",           # Reasoning, nuanced tasks
        "reasoning": "o1"             # Math, logic problems
    }
    return routing[task_complexity]

model = smart_model_selection(analyze_complexity(task))
```

**When to use:** Variable task complexity, cost optimization
**Expected impact:** 40-60% cost savings vs. always using premium model

---

### Performance Optimization:

**Speed:**
- Use parallel processing (ThreadPoolExecutor) for batch operations
- Implement caching (Redis) for repeated requests
- Use streaming for real-time UI (perceived speed improvement)
- Choose gpt-5-mini for simple tasks (3x faster response time)

**Quality:**
- Use GPT-5 for complex reasoning, creative content, nuanced understanding
- Lower temperature (0.0-0.3) for consistency
- Provide detailed system instructions with examples
- Use o1 model for mathematical, logical, or analytical tasks

**Cost:**
- Cache responses (30-50% savings)
- Use gpt-5-mini when possible (97% cheaper than GPT-5)
- Reduce max_tokens (shorter = cheaper)
- Set per-user rate limits (prevent runaway costs)
- Implement usage-based pricing for customers (pass costs through)

---

## ðŸŽ¯ Your Action Plan

### This Week: Build Your Foundation
**Total Time: 2-3 hours**

**Day 1: Get API Access**
- [ ] Create OpenAI account at platform.openai.com (5 min)
- [ ] Add payment method and set $100 spending limit (5 min)
- [ ] Generate API key and store securely (5 min)
- [ ] Test first API call using Quick Win code (15 min)
- [ ] Verify you can see usage at platform.openai.com/usage (5 min)

**Day 2-3: Build First Integration**
- [ ] Identify one high-volume repetitive task (30 min)
- [ ] Choose appropriate production template (15 min)
- [ ] Adapt code to your specific use case (1 hr)
- [ ] Test with 5-10 real examples (30 min)
- [ ] Calculate cost per request and time saved (15 min)

**Week 1 Success Metric:** One working API integration processing real tasks; documented cost and time savings

---

### This Month: Achieve Mastery
**Total Time: 15-20 hours**

**Week 1: Production-Ready Integration**
- Goal: Build reliable, error-handled integration
- Actions:
  - Add retry logic and error handling
  - Implement usage tracking
  - Set up monitoring/alerts
  - Test edge cases and failures
- Outcome: Robust integration ready for production volume

**Week 2: Optimization**
- Goal: Reduce costs and improve performance
- Actions:
  - Implement response caching
  - Optimize model selection (use gpt-5-mini where possible)
  - Add rate limiting
  - Measure and optimize for speed
- Outcome: 30-50% cost reduction, faster performance

**Week 3: Scale to Multiple Use Cases**
- Goal: Expand to 3-5 integrations
- Actions:
  - Identify 4 more automation opportunities
  - Build using production templates
  - Reuse code patterns from first integration
  - Document each integration's ROI
- Outcome: 5 API integrations handling significant volume

**Week 4: Team Enablement**
- Goal: Enable colleagues to leverage API
- Actions:
  - Create shared code library/wrapper
  - Document common patterns and templates
  - Train 2-3 technical colleagues
  - Measure aggregate time/cost savings
- Outcome: Team capable of building API integrations; measurable ROI

**Month 1 Success Metric:** 5+ working integrations; 100+ hours/month time saved; 10:1 ROI (savings vs. costs)

---

### 90-Day Transformation: Expert Level

**Month 1: Individual API Mastery**
You've built 5 API integrations handling repetitive tasks. You understand request structure, parameters, error handling, and cost optimization. You're saving 20+ hours/week through automation. You can estimate costs and ROI for new use cases.

**Month 2: Advanced Patterns & Team Scale**
You've implemented caching, rate limiting, and usage-based billing. You've built enterprise-grade integrations with monitoring and failover. Your team has adopted API-powered automations across multiple departments. You're processing thousands of API calls per day reliably.

**Month 3: Strategic AI Architecture**
You're designing multi-stage AI pipelines and product features powered by GPT APIs. You understand model selection strategy, cost control at scale, and integration patterns. You're advising on AI product roadmap and vendor selection. You can build AI-powered products from scratch.

**Final Outcome:** You've transformed from API beginner to AI integration architect. You can design, build, and deploy scalable API-powered automations and product features. You understand enterprise considerations: cost control, reliability, monitoring, security.

**Certification Ready:** You can demonstrate:
- Portfolio of 10+ API integrations solving real problems
- Enterprise-grade code (error handling, monitoring, caching)
- Cost optimization techniques (50%+ savings vs. naive implementation)
- Measured ROI (time/cost saved documented)
- Team enablement (others using your code/patterns)

---

**Next Lesson:** Lesson 9 - ChatGPT Enterprise & Teams ðŸš€
